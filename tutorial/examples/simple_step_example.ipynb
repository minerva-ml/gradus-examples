{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import steps\n",
    "from steps.base import Step, BaseTransformer, hstack_inputs\n",
    "from steps.sklearn.models import make_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "import sklearn.preprocessing as prep \n",
    "from sklearn.ensemble import RandomForestRegressor as RFR\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class MinMaxScaler(BaseTransformer):\n",
    "    def __init__(self):\n",
    "        self.scaler = prep.MinMaxScaler()\n",
    "        \n",
    "    def fit(self, X):\n",
    "        self.scaler.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_  = self.scaler.transform(X)\n",
    "        return {'X':X_}\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        joblib.dump(self.scaler, filepath)\n",
    "        \n",
    "    def load(self, filepath):\n",
    "        self.scaler = joblib.load(filepath)\n",
    "        return self\n",
    "    \n",
    "class Normalizer(BaseTransformer):\n",
    "    def __init__(self):\n",
    "        self.scaler = prep.Normalizer()\n",
    "        \n",
    "    def fit(self, X):\n",
    "        self.scaler.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_  = self.scaler.transform(X)\n",
    "        return {'X':X_}\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        joblib.dump(self.scaler, filepath)\n",
    "        \n",
    "    def load(self, filepath):\n",
    "        self.scaler = joblib.load(filepath)\n",
    "        return self\n",
    "    \n",
    "class RandomForest(BaseTransformer):\n",
    "    def __init__(self):\n",
    "        self.estimator = RFR()\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.estimator.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, **kwargs):\n",
    "        y_pred  = self.estimator.predict(X)\n",
    "        return {'y_pred':y_pred}\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        joblib.dump(self.estimator, filepath)\n",
    "        \n",
    "    def load(self, filepath):\n",
    "        self.estimator = joblib.load(filepath)\n",
    "        return self\n",
    "    \n",
    "def hstack_vector_inputs(inputs):\n",
    "    inputs_ = [input_.reshape(-1,1) for input_ in inputs]\n",
    "    return np.hstack(inputs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = load_diabetes(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /mnt/ml-team/minerva/debug/example_problem/outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = '/mnt/ml-team/minerva/debug/example_problem'\n",
    "\n",
    "scaler = Step(name='scaler',\n",
    "                  transformer=MinMaxScaler(),\n",
    "                  input_data=['input'],\n",
    "                  adapter={'X':[('input','X')]},\n",
    "                  cache_dirpath=CACHE_DIR\n",
    "                 )\n",
    "\n",
    "normalizer = Step(name='normalizer',\n",
    "                  transformer=Normalizer(),\n",
    "                  input_data=['input'],\n",
    "                  adapter={'X':[('input','X')]},\n",
    "                  cache_dirpath=CACHE_DIR,\n",
    "                  cache_output=True\n",
    "                 )\n",
    "\n",
    "classifer = Step(name='clf',\n",
    "                 transformer=RandomForest(),\n",
    "                 input_data=['input'],\n",
    "                 input_steps=[scaler, normalizer],                 \n",
    "                 adapter={'y':([('input','y')]),\n",
    "                          'X':([('scaler','X'),\n",
    "                               ('normalizer','X')], hstack_inputs)\n",
    "                         },\n",
    "                 cache_dirpath=CACHE_DIR\n",
    "                )\n",
    "\n",
    "scaler1 = Step(name='scaler1',\n",
    "                  transformer=MinMaxScaler(),\n",
    "                  input_data=['input'],\n",
    "                  adapter={'X':[('input','X')]},\n",
    "                  cache_dirpath=CACHE_DIR\n",
    "                 )\n",
    "\n",
    "normalizer = Step(name='normalizer',\n",
    "                  transformer=Normalizer(),\n",
    "                  input_data=['input'],\n",
    "                  adapter={'X':[('input','X')]},\n",
    "                  cache_dirpath=CACHE_DIR\n",
    "                 )\n",
    "\n",
    "classifer1 = Step(name='clf1',\n",
    "                 transformer=RandomForest(),\n",
    "                 input_data=['input'],\n",
    "                 input_steps=[scaler1, normalizer],                 \n",
    "                 adapter={'y':([('input','y')]),\n",
    "                          'X':([('scaler1','X'),\n",
    "                               ('normalizer','X')], hstack_inputs)\n",
    "                         },\n",
    "                 cache_dirpath=CACHE_DIR\n",
    "                )\n",
    "\n",
    "ensemble = Step(name='ensemble',\n",
    "                 transformer=RandomForest(),\n",
    "                 input_data=['input'],\n",
    "                 input_steps=[classifer, classifer1],                 \n",
    "                 adapter={'y':([('input','y')]),\n",
    "                          'X':([('clf','y_pred'),\n",
    "                               ('clf1','y_pred')], hstack_vector_inputs)\n",
    "                         },\n",
    "                 cache_dirpath=CACHE_DIR,\n",
    "                force_fitting=True\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'input': {'X': X,\n",
    "                  'y': y,\n",
    "                 },\n",
    "            }\n",
    "\n",
    "ensemble.clean_cache()\n",
    "output = ensemble.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output['y_pred'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /mnt/ml-team/minerva/debug/example_problem/outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = joblib.load('/mnt/ml-team/minerva/debug/example_problem/outputs/clf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf['y_pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpu py3",
   "language": "python",
   "name": "cpu_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
